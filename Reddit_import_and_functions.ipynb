{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import pos_tag, word_tokenize\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import requests\n",
    "import datetime, time\n",
    "import h5py\n",
    "\n",
    "import re \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, LassoCV, RidgeCV\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.display.max_columns = 90\n",
    "pd.options.display.max_rows = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Project's Exclusion Rules, Stop Words and Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section works through step by step all sub-sets of Stop Words to exclude during pre-processing of both subreddit data.\n",
    "\n",
    "Words deemed unimportant to the meaning of posts or to differentiation between the two subreddits will be filtered out during pre-processing. Regex will also be used during final dataframe creation to filter out other stop words. General rules and systemization for stop words is notated below: \n",
    "\n",
    "1. All words with Word Vectorize counts less than 5 will be dropped. This is before lemmatization, due to the processing time constraints for the long tail of words and given the size of the data sets. After dropping these extremely rare words, the entire corpus will be lemmatized and recombined. Given words might have 2, or maybe 3 or 4 forms of the same root word so the absolute upper desired threshold is really 16 occurences, 4 occurences in a max of 4 different forms should be dropped. TF-IDF Vectorize will be tested, theorized to be most useful for proper nouns, as well as part of speech analysis using word_tokenize.\n",
    "\n",
    "2. All numbers are dropped. Some like dates have good meaning but not in terms of differentiating between most all subreddit topics.\n",
    "3. All special HTML character entities are dropped (ex: &nbsp, &amp etc), non-alphabet characters are dropped and all punctuation is dropped. Before processing this, all apostrophized/contraction words are dropped, since these basically all fall in the connecting word category.\n",
    "4. All single letter words are dropped, many will be created from dropping punctuation\n",
    "5. All two letter words are dropped. Almost all are connecting words which fall into or are functionally similar to the English Stop Word set. Many in the project data may be abbreviations with some value in keeping, but without surrounding word context it seems impossible to derive their meaning. The ones in the dictionary are below, few are colloquially meaningful and none seem of any use in classifying non-niche reddit topics (excluding maybe music):\n",
    "\n",
    "| Two Letter Words in English Dictionary                       |\n",
    "| ------------------------------------------------------------ |\n",
    "| AA, AB, AD, AE, AG, AH, AI, AL, AM, AN, AR, AS, AT, AW, AX, AY |\n",
    "| BA, BE, BI, BO, BY, CH, DA, DE, DI, DO                       |\n",
    "| EA, ED, EE, EF, EH, EL, EM, EN, ER, ES, ET, EW, EX           |\n",
    "| FA, FE, FY, GI, GO, GU, HA, HE, HI, HM, HO                   |\n",
    "| ID, IF, IN, IO, IS, IT, JA, JO                               |\n",
    "| KA, KI, KO, KY, LA, LI, LO                                   |\n",
    "| MA, ME, MI, MM, MO, MU, MY, NA, NE, NO, NU, NY               |\n",
    "| OB, OD, OE, OF, OH, OI,OK, OM, ON, OO, OP, OR, OS, OU, OW, OX, OY |\n",
    "| PA, PE, PI, PO, QI                                           |\n",
    "| RE, SH, SI, SO, ST, TA, TE, TI, TO                           |\n",
    "| UG, UH, UM, UN, UP, UR, US, UT                               |\n",
    "| WE, WO, XI, XU, YA, YE, YO, YU, ZA                           |\n",
    "\n",
    "6. All lemmatized roots of words of the SKLearn English Stop Word set will be dropped.\n",
    "7. All post URLs will be dropped. Initially, hope was to keep keywords but it is too hard to differentiate keywords from all others in url and there's too much junk and also undescriptive names. Only the main post attribute url will be split, due to a significant group of posts only having a title and a link to a main information webpage.\n",
    "7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "excludedWords = set([lem.lemmatize(word) for word in stop_words.ENGLISH_STOP_WORDS])\n",
    "\n",
    "# Using set functionality in case add same word twice to excludedWords and adds runtime efficiency\n",
    "myStopWords = {'com','deleted'}\n",
    "# redd probably comes from reddit truncated during processing, wouldn't differentiate subreddits\n",
    "# Actually, won't stop word redd since one subreddit might refer to reddit more often\n",
    "\n",
    "# Could drop rare frequency un-interpretable non-words\n",
    "# possible: ezbatteriesreconditioning (15) seems like ad/spam, ambrosusamb (15) but could be users\n",
    "# ~200 lowest frequency of 15 words, looks like that min_df cut out most complete non-words\n",
    "\n",
    "excludedWords = excludedWords.union(myStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printname(name) :\n",
    "   print(name)\n",
    "\n",
    "def list_all_hdf5(file) :\n",
    "    with h5py.File(file, 'r') as f:\n",
    "       f.visit(printname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decodeText(matchobj) :  # excludes all 2 letter 'words' and url specific keywords\n",
    "    if type(matchobj) == str : text = matchobj\n",
    "    else : text = matchobj.group(0)\n",
    "    text = re.sub(r'\\d+\\w+\\d+|\\d+',' ',text)\n",
    "    match = re.match(r'\\W(\\w{1,2})\\W', text)\n",
    "    if match : return ' '\n",
    "    if re.match(r\"http\", text) :\n",
    "        urltypetext = r'https{0,1}://|www\\.|comments|reddit|\\.com/r|\\.com|/CryptoCurrency|/environment|html'\n",
    "        punct = r'|-|\\.|\\$|@|\\?|\\%|\\:|\\;|\\(|\\)|\\=|\\[|\\]|\\{|\\}|,'\n",
    "        return re.sub(urltypetext + punct, ' ', text)        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' //blog zerion io/defi and trading assets decentralized exchanges  c]( //blog zerion io/defi and trading assets decentralized exchanges  c)'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testurl = 'https://www.reddit.com/r/CryptoCurrency/comments/7menr1/myriad_seems_to_be_forming_an_inverse_head_and/'\n",
    "testurl = 'https://blog.zerion.io/defi-and-trading-assets-decentralized-exchanges-26543eda7c](https://blog.zerion.io/defi-and-trading-assets-decentralized-exchanges-26543eda7c)'\n",
    "decodeText(testurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitText(text) :\n",
    "    text = re.sub(r'\\W\\w{1,2}\\W|http\\S+',decodeText, text)\n",
    "    midtext = re.sub(r'_|\\W\\w{1,2}\\W|\\d+|&\\w+|\\#\\w+|\\w+\\'\\w',' ', text)  \n",
    "    # filters out \"'s\" and other apostrophe words, 1st pass of less than 3 char words\n",
    "    wordlist = re.findall(r'(\\w{3,})', midtext)  \n",
    "    # findAll drops all words that are 2 or fewer characters as well as all punctuation\n",
    "    return wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtext = '''A big obstacle for decentralization of trade marketplaces is how to deal with order \n",
    "books. They are lists where traders post buy and sell requests.\\r\\n\\r\\nIndividual users and especially \n",
    "market makers\\u200a—\\u200atraders who bring liquidity to markets and put buy and sell orders for many \n",
    "different assets or tokens\\u200a—\\u200aadd or remove orders constantly. The sheer amount of trading \n",
    "orders is too much for an on-chain solution to handle.\\r\\n\\r\\nThe first intermediary solution is \n",
    "Bancor’s smart token, which operates the exchange itself.\\r\\n\\r\\n&gt;*Bancor not only eliminates the \n",
    "third-party broker, but also the exchange trading\\xa0partner.*\\r\\n\\r\\nTo exchange a token, a user \n",
    "only needs to interact with a smart contract that holds tokens in reserve, and so trades do not need \n",
    "to rely on the presence of other offers.\\r\\n\\r\\nHowever, the Bancor protocol encodes and influences \n",
    "the price discovery algorithmically within its own tokens, which limits liquidity and market \n",
    "efficiency.  \\r\\n[https://blog.zerion.io/defi-and-trading-assets-decentralized-exchanges-26543eda7c]\n",
    "(https://blog.zerion.io/defi-and-trading-assets-decentralized-exchanges-26543eda7c)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtext2 = \"\"\"[Learn about NIX lpos](https://medium.com/@nixplatform/nixs-leasing-proof-of-stake-consensus-837fe083de4f)\n",
    "\n",
    "&amp;#x200B;\n",
    "\n",
    "Leasing proof of stake is now active on the NIX Platform network. With LPoS you will be able to lease your coins to merchants or other third parties so they can stake on your behalf and claim a percentage reward you allow in the initial contract. For example, party A leases 500 nix to party B allowing a 5% fee reward. Party B can now stake those 500 coins, when party B hits a stake, they are allowed to take 5% of the stake reward leaving party A to now own 500 + stake reward - 5% of stake reward. Party B can continue to stake those coins as long as the leasing contract is active. Party A can void that contract whenever he or she wants.  And to be clear, Party A can make a leasing contract that takes 0% fee, this can be used as a simple cold staking script if he or she wants to cold stake via their own computer. \n",
    "\n",
    "So this is fully trustless cold staking. You can keep your coins locked away on soon to be ledger and/or trezor while leasing them to yourself or any merchant with 0 risk. You control the leasing contract and can stop it at anytime. Whoever you lease the coins too can only stake them. You coins are stored offline safely, while also allowing you to earn stake rewards by leasing them in a fully trustless manner. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtext3 = \"\"\" Today we get underway with arguably one of the most prominent and respected events in\n",
    "the crypto space — **Token Summit**, this time from Silicon Valley.  Bluenote is on hand to present \n",
    "our vision of a game-changing solution for combating climate change with blockchain — and we’ll be \n",
    "updating this post with content, pictures and quotes from throughout the day!  Don’t miss out!  \n",
    "&amp;#x200B;  *Processing img 24b4biwbav421...*     Just about to get started!  1. Question from \n",
    "William Mougayar to Fred Ehrsam (Coinbase) “What gives a token long-term value” **Answer: “It goes \n",
    "back to the team — can they execute…and are they executing already?”** 2. A great crowd in UCSF!  \n",
    "&amp;#x200B;  *Processing img 8gvsw0zdav421...*   3. Fireside chat with William Mougayar and Oliver \n",
    "Bussmann — Crypto Valley meets Silicon Valley   &amp;#x200B;  *Processing img ox7ianmfav421...*   \n",
    "4. A brief intro to Bluenote — Michiel Frackers showed off the Bluenote protocol on the main stage.   \n",
    "&amp;#x200B;  *Processing img zluxym6hav421...*     Thanks for following along with us during this \n",
    "exciting day!  *Follow us on* [*Twitter*](https://twitter.com/bluenote_world)*,* [*LinkedIn*]\n",
    "(https://www.linkedin.com/company/11350676/admin/updates/) *and* [*Facebook*](https://www.facebook.com\n",
    "/bluenote.world/)*!*  Learn more:  [https://bluenote.world/](https://bluenote.world/) \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Today get underway with arguably one the most prominent and respected events the crypto space — **Token Summit**, this time from Silicon Valley.  Bluenote hand present our vision game-changing solution for combating climate change with blockchain — and updating this post with content, pictures and quotes from throughout the day!  Don miss out!   ; ;  *Processing img  b biwbav ...*     Just about get started!   . Question from William Mougayar Fred Ehrsam (Coinbase) “What gives token long-term value” **Answer:  goes back the team — can they execute…and are they executing already?”**  . great crowd UCSF!   ; ;  *Processing img  gvsw zdav ...*    . Fireside chat with William Mougayar and Oliver Bussmann — Crypto Valley meets Silicon Valley    ; ;  *Processing img ox ianmfav ...*    . brief intro Bluenote — Michiel Frackers showed off the Bluenote protocol the main stage.    ; ;  *Processing img zluxym hav ...*     Thanks for following along with during this exciting day!  *Follow  [*Twitter*]( //twitter com/bluenote world)*,* [*LinkedIn*]( // linkedin com/company/ /admin/updates/) *and* [*Facebook*]( // facebook com/bluenote world/)*!*  Learn more:  [ //bluenote world/]( //bluenote world/) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Today',\n",
       " 'get',\n",
       " 'underway',\n",
       " 'with',\n",
       " 'arguably',\n",
       " 'one',\n",
       " 'the',\n",
       " 'most',\n",
       " 'prominent',\n",
       " 'and',\n",
       " 'respected',\n",
       " 'events',\n",
       " 'the',\n",
       " 'crypto',\n",
       " 'space',\n",
       " 'Token',\n",
       " 'Summit',\n",
       " 'this',\n",
       " 'time',\n",
       " 'from',\n",
       " 'Silicon',\n",
       " 'Valley',\n",
       " 'Bluenote',\n",
       " 'hand',\n",
       " 'present',\n",
       " 'our',\n",
       " 'vision',\n",
       " 'game',\n",
       " 'changing',\n",
       " 'solution',\n",
       " 'for',\n",
       " 'combating',\n",
       " 'climate',\n",
       " 'change',\n",
       " 'with',\n",
       " 'blockchain',\n",
       " 'and',\n",
       " 'updating',\n",
       " 'this',\n",
       " 'post',\n",
       " 'with',\n",
       " 'content',\n",
       " 'pictures',\n",
       " 'and',\n",
       " 'quotes',\n",
       " 'from',\n",
       " 'throughout',\n",
       " 'the',\n",
       " 'day',\n",
       " 'Don',\n",
       " 'miss',\n",
       " 'out',\n",
       " 'Processing',\n",
       " 'img',\n",
       " 'biwbav',\n",
       " 'Just',\n",
       " 'about',\n",
       " 'get',\n",
       " 'started',\n",
       " 'Question',\n",
       " 'from',\n",
       " 'William',\n",
       " 'Mougayar',\n",
       " 'Fred',\n",
       " 'Ehrsam',\n",
       " 'Coinbase',\n",
       " 'What',\n",
       " 'gives',\n",
       " 'token',\n",
       " 'long',\n",
       " 'term',\n",
       " 'value',\n",
       " 'Answer',\n",
       " 'goes',\n",
       " 'back',\n",
       " 'the',\n",
       " 'team',\n",
       " 'can',\n",
       " 'they',\n",
       " 'execute',\n",
       " 'and',\n",
       " 'are',\n",
       " 'they',\n",
       " 'executing',\n",
       " 'already',\n",
       " 'great',\n",
       " 'crowd',\n",
       " 'UCSF',\n",
       " 'Processing',\n",
       " 'img',\n",
       " 'gvsw',\n",
       " 'zdav',\n",
       " 'Fireside',\n",
       " 'chat',\n",
       " 'with',\n",
       " 'William',\n",
       " 'Mougayar',\n",
       " 'and',\n",
       " 'Oliver',\n",
       " 'Bussmann',\n",
       " 'Crypto',\n",
       " 'Valley',\n",
       " 'meets',\n",
       " 'Silicon',\n",
       " 'Valley',\n",
       " 'Processing',\n",
       " 'img',\n",
       " 'ianmfav',\n",
       " 'brief',\n",
       " 'intro',\n",
       " 'Bluenote',\n",
       " 'Michiel',\n",
       " 'Frackers',\n",
       " 'showed',\n",
       " 'off',\n",
       " 'the',\n",
       " 'Bluenote',\n",
       " 'protocol',\n",
       " 'the',\n",
       " 'main',\n",
       " 'stage',\n",
       " 'Processing',\n",
       " 'img',\n",
       " 'zluxym',\n",
       " 'hav',\n",
       " 'Thanks',\n",
       " 'for',\n",
       " 'following',\n",
       " 'along',\n",
       " 'with',\n",
       " 'during',\n",
       " 'this',\n",
       " 'exciting',\n",
       " 'day',\n",
       " 'Follow',\n",
       " 'Twitter',\n",
       " 'twitter',\n",
       " 'com',\n",
       " 'bluenote',\n",
       " 'world',\n",
       " 'LinkedIn',\n",
       " 'linkedin',\n",
       " 'com',\n",
       " 'company',\n",
       " 'admin',\n",
       " 'updates',\n",
       " 'and',\n",
       " 'Facebook',\n",
       " 'facebook',\n",
       " 'com',\n",
       " 'bluenote',\n",
       " 'world',\n",
       " 'Learn',\n",
       " 'more',\n",
       " 'bluenote',\n",
       " 'world',\n",
       " 'bluenote',\n",
       " 'world']"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitText(testtext3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('cryptocurrency12_12_18to10_29_18.csv')\n",
    "# df = pd.read_csv('cryptocurrency10_29_18to9_9_18.csv')\n",
    "# df = pd.read_csv('cryptocurrency9_9_18to7_21_18.csv')\n",
    "# df = pd.read_csv('cryptocurrency7_20_18to6_18_18.csv')\n",
    "# df = pd.read_csv('cryptocurrency6_17_18to5_16_18.csv')\n",
    "# df = pd.read_csv('cryptocurrency5_15_18to4_13_18.csv')\n",
    "# df = pd.read_csv('cryptocurrency4_12_18to3_11_18.csv')\n",
    "# df = pd.read_csv('cryptocurrency3_10_18to2_6_18.csv')\n",
    "# df = pd.read_csv('cryptocurrency2_5_18to1_12_18.csv')\n",
    "# df = pd.read_csv('cryptocurrency1_12_18to12_27_17.csv')\n",
    "# df = pd.read_csv('cryptocurrency12_26_17to9_4_17.csv')\n",
    "# df = pd.read_csv('environmentFinal.csv',na_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testtext = df.loc[39,'selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 195717 entries, 0 to 195716\n",
      "Data columns (total 9 columns):\n",
      "Unnamed: 0    195717 non-null int64\n",
      "date          195717 non-null object\n",
      "permalink     195717 non-null object\n",
      "subreddit     195717 non-null object\n",
      "url           195717 non-null object\n",
      "title         195716 non-null object\n",
      "author        195717 non-null object\n",
      "selftext      13836 non-null object\n",
      "id            195717 non-null object\n",
      "dtypes: int64(1), object(8)\n",
      "memory usage: 13.4+ MB\n"
     ]
    }
   ],
   "source": [
    "#df.info()  # environmentFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 429116 entries, 0 to 429115\n",
      "Data columns (total 9 columns):\n",
      "Unnamed: 0      429116 non-null int64\n",
      "Unnamed: 0.1    429116 non-null object\n",
      "date            429115 non-null object\n",
      "full_link       429115 non-null object\n",
      "subreddit       429115 non-null object\n",
      "url             429115 non-null object\n",
      "title           429115 non-null object\n",
      "author          429115 non-null object\n",
      "selftext        177171 non-null object\n",
      "dtypes: int64(1), object(8)\n",
      "memory usage: 29.5+ MB\n"
     ]
    }
   ],
   "source": [
    "#df.info() # cryptocurrencyFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>date</th>\n",
       "      <th>full_link</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>429111</th>\n",
       "      <td>89223</td>\n",
       "      <td>6xw0np</td>\n",
       "      <td>(2017, 9, 4, 21, 38)</td>\n",
       "      <td>https://www.reddit.com/r/CryptoCurrency/commen...</td>\n",
       "      <td>CryptoCurrency</td>\n",
       "      <td>https://www.reddit.com/r/CryptoCurrency/commen...</td>\n",
       "      <td>What's the cheapest masternode I can get</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>[removed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429112</th>\n",
       "      <td>89224</td>\n",
       "      <td>6xw26k</td>\n",
       "      <td>(2017, 9, 4, 22, 32)</td>\n",
       "      <td>https://www.reddit.com/r/CryptoCurrency/commen...</td>\n",
       "      <td>CryptoCurrency</td>\n",
       "      <td>https://www.reddit.com/r/CryptoCurrency/commen...</td>\n",
       "      <td>Anyone else heard of the new altcoinexchange?</td>\n",
       "      <td>snittolo</td>\n",
       "      <td>Noticing a lot of the names staring to appear ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429113</th>\n",
       "      <td>89225</td>\n",
       "      <td>6xw340</td>\n",
       "      <td>(2017, 9, 4, 22, 46)</td>\n",
       "      <td>https://www.reddit.com/r/CryptoCurrency/commen...</td>\n",
       "      <td>CryptoCurrency</td>\n",
       "      <td>https://i.redd.it/p6g2g5u3pqjz.jpg</td>\n",
       "      <td>WTF?</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>[deleted]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429114</th>\n",
       "      <td>89226</td>\n",
       "      <td>6xw3zs</td>\n",
       "      <td>(2017, 9, 4, 22, 53)</td>\n",
       "      <td>https://www.reddit.com/r/CryptoCurrency/commen...</td>\n",
       "      <td>CryptoCurrency</td>\n",
       "      <td>https://i.redd.it/w92kgl7wpqjz.png</td>\n",
       "      <td>Multi-Millionaire Dan Bilzerian into Cryptocur...</td>\n",
       "      <td>bizshawn</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429115</th>\n",
       "      <td>89227</td>\n",
       "      <td>6xw452</td>\n",
       "      <td>(2017, 9, 4, 22, 35)</td>\n",
       "      <td>https://www.reddit.com/r/CryptoCurrency/commen...</td>\n",
       "      <td>CryptoCurrency</td>\n",
       "      <td>https://www.gotmonero.com/</td>\n",
       "      <td>Forget Milk... Got Monero?</td>\n",
       "      <td>Bitcoinfriend</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0 Unnamed: 0.1                  date  \\\n",
       "429111       89223       6xw0np  (2017, 9, 4, 21, 38)   \n",
       "429112       89224       6xw26k  (2017, 9, 4, 22, 32)   \n",
       "429113       89225       6xw340  (2017, 9, 4, 22, 46)   \n",
       "429114       89226       6xw3zs  (2017, 9, 4, 22, 53)   \n",
       "429115       89227       6xw452  (2017, 9, 4, 22, 35)   \n",
       "\n",
       "                                                full_link       subreddit  \\\n",
       "429111  https://www.reddit.com/r/CryptoCurrency/commen...  CryptoCurrency   \n",
       "429112  https://www.reddit.com/r/CryptoCurrency/commen...  CryptoCurrency   \n",
       "429113  https://www.reddit.com/r/CryptoCurrency/commen...  CryptoCurrency   \n",
       "429114  https://www.reddit.com/r/CryptoCurrency/commen...  CryptoCurrency   \n",
       "429115  https://www.reddit.com/r/CryptoCurrency/commen...  CryptoCurrency   \n",
       "\n",
       "                                                      url  \\\n",
       "429111  https://www.reddit.com/r/CryptoCurrency/commen...   \n",
       "429112  https://www.reddit.com/r/CryptoCurrency/commen...   \n",
       "429113                 https://i.redd.it/p6g2g5u3pqjz.jpg   \n",
       "429114                 https://i.redd.it/w92kgl7wpqjz.png   \n",
       "429115                         https://www.gotmonero.com/   \n",
       "\n",
       "                                                    title         author  \\\n",
       "429111           What's the cheapest masternode I can get      [deleted]   \n",
       "429112      Anyone else heard of the new altcoinexchange?       snittolo   \n",
       "429113                                               WTF?      [deleted]   \n",
       "429114  Multi-Millionaire Dan Bilzerian into Cryptocur...       bizshawn   \n",
       "429115                         Forget Milk... Got Monero?  Bitcoinfriend   \n",
       "\n",
       "                                                 selftext  \n",
       "429111                                          [removed]  \n",
       "429112  Noticing a lot of the names staring to appear ...  \n",
       "429113                                          [deleted]  \n",
       "429114                                                NaN  \n",
       "429115                                                NaN  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('cryptocurrencyFinal.csv', mode='a', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Legacy function, trying to use Reddit API to get enough posts, but on server side they now only\n",
    "# allow access to 1st 1000 posts no matter what sorted listing you try to retrieve\n",
    "# Reddit API not used in favor of PushShift API\n",
    "def add1000posts(url, posts, headers={}, params={}) : # leaves posts as raw json dicts\n",
    "    after = None\n",
    "    if params == {} : params = {'after' : after, 'count' : 0}\n",
    "    for i in range(40) :  # 25 posts at a time, 40 times for 1000 post limit\n",
    "        res = requests.get(url, params=params, headers=headers)\n",
    "        if res.status_code == 200 :\n",
    "            json1 = res.json()\n",
    "            posts.extend(json1['data']['children'])\n",
    "            after = json1['data']['after']\n",
    "            params['after'] = after\n",
    "            params['count'] += 25\n",
    "        else :\n",
    "            print(res.status_code)\n",
    "            break\n",
    "        time.sleep(1)\n",
    "        \n",
    "    return (posts, params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
