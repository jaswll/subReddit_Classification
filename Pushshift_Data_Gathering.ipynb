{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 429116 entries, 0 to 429115\n",
      "Data columns (total 9 columns):\n",
      "Unnamed: 0      429116 non-null int64\n",
      "Unnamed: 0.1    429116 non-null object\n",
      "date            429115 non-null object\n",
      "full_link       429115 non-null object\n",
      "subreddit       429115 non-null object\n",
      "url             429115 non-null object\n",
      "title           429115 non-null object\n",
      "author          429115 non-null object\n",
      "selftext        177171 non-null object\n",
      "dtypes: int64(1), object(8)\n",
      "memory usage: 29.5+ MB\n"
     ]
    }
   ],
   "source": [
    "%run Reddit_import_and_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code:  200\n"
     ]
    }
   ],
   "source": [
    "subreddit1 = 'Environment'\n",
    "subreddit2 = 'Cryptocurrency'\n",
    "headers = {'User-agent' : 'General Assembly Project 3-J'}\n",
    "#zerotime = int(datetime.datetime.utcnow().timestamp())\n",
    "zerotime = 1545098409 # will grab by each previous hour\n",
    "params = {'size':500,'subreddit':subreddit1,'after':(zerotime-60*60*24*3),'before':zerotime}\n",
    "# Pulling posts starting from 2 years ago onwards, 2 days per pull to stay under 500 post limit\n",
    "\n",
    "url = 'https://api.pushshift.io/reddit/search/submission/'\n",
    "response = requests.get(url, headers=headers, params=params)\n",
    "print('Status Code: ',response.status_code)  # testing if connection is accepting my IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['date','permalink','subreddit','url','title','author','selftext'] \n",
    "# date is only column not directly in json\n",
    "\n",
    "df = pd.DataFrame(columns = features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsontocsv(json, csvfile) : # efficient, fastest & stores data at each step (better than jsontodf())\n",
    "                               # will re-index based on post id when re-create dataframe\n",
    "    df = pd.DataFrame(columns = features)\n",
    "    for post_dict in json['data'] :\n",
    "        \n",
    "        date_tuple = datetime.datetime.utcfromtimestamp(post_dict['created_utc'])\n",
    "        year = date_tuple.year\n",
    "        month = date_tuple.month\n",
    "        day = date_tuple.day\n",
    "        hour = date_tuple.hour\n",
    "        second = date_tuple.second\n",
    "        try : post_dict['selftext']\n",
    "        except KeyError : post_dict['selftext'] = ''\n",
    "        \n",
    "        df_dict = {feature : post_dict[feature] for feature in features[1:]}\n",
    "        df_dict['date'] = (year, month, day, hour, second)\n",
    "        df_dict['id'] = post_dict['id']\n",
    "        df_dict['url'] = df_dict['url'].replace(',','').replace(';','')\n",
    "        \n",
    "        df = df.append(df_dict, ignore_index=True)\n",
    "        \n",
    "    df.to_csv(csvfile, mode='a', header=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Older version of json converting function, csv version is much faster\n",
    "def jsontodf(json, df) :\n",
    "    for post_dict in json['data'] :\n",
    "        \n",
    "        date_tuple = datetime.datetime.utcfromtimestamp(post_dict['retrieved_on'])\n",
    "        year = date_tuple.year\n",
    "        month = date_tuple.month\n",
    "        day = date_tuple.day\n",
    "        hour = date_tuple.hour\n",
    "        second = date_tuple.second\n",
    "        post_dict['date'] = (year, month, day, hour, second)\n",
    "        \n",
    "        for feature in features :  # dataframe indexed by reddit's post id\n",
    "            try :                  # this ensures no duplicates, as inputting same post again will just\n",
    "                                   # overwrite exact oldy copy\n",
    "                df.loc[post_dict['id'], feature] = post_dict[feature] \n",
    "            except KeyError :\n",
    "                df.loc[post_dict['id'], feature] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addposts(url, params, df=pd.DataFrame(), csvfile=None, headers={}) : # adds all posts to df unless over 500 in one pull\n",
    "    #numrows3 = 0\n",
    "    for i in range(1*122) :  # 8 days per call, max 96x500=48,000 posts pulled\n",
    "        res = requests.get(url, params=params, headers=headers)\n",
    "        if res.status_code == 200 :\n",
    "            #numrows, _ = df.shape\n",
    "            #jsontodf(res.json(), df)\n",
    "            df = jsontocsv(res.json(), csvfile)\n",
    "            #numrows2, _ = df.shape\n",
    "            if (i % 1) == 0 :\n",
    "                #print(numrows2-numrows, numrows-numrows3, 'rows to df', datetime.datetime.utcfromtimestamp(params['before']))\n",
    "                print(df.shape[0], 'rows to df', datetime.datetime.utcfromtimestamp(params['before']))\n",
    "            #numrows3 = numrows\n",
    "            params['after'] = params['after']-60*60*24*3\n",
    "            params['before'] = params['before']-60*60*24*3\n",
    "        else :\n",
    "            print(res.status_code)\n",
    "            break\n",
    "        time.sleep(0.1)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': 'neerjsn',\n",
       " 'author_flair_css_class': None,\n",
       " 'author_flair_richtext': [],\n",
       " 'author_flair_text': None,\n",
       " 'author_flair_type': 'text',\n",
       " 'author_fullname': 't2_2oen7oyu',\n",
       " 'author_patreon_flair': False,\n",
       " 'can_mod_post': False,\n",
       " 'contest_mode': False,\n",
       " 'created_utc': 1545061541,\n",
       " 'domain': 'i.redd.it',\n",
       " 'full_link': 'https://www.reddit.com/r/environment/comments/a70w7w/top_10_thing_you_might_not_know_about_pollution/',\n",
       " 'gildings': {'gid_1': 0, 'gid_2': 0, 'gid_3': 0},\n",
       " 'id': 'a70w7w',\n",
       " 'is_crosspostable': False,\n",
       " 'is_meta': False,\n",
       " 'is_original_content': False,\n",
       " 'is_reddit_media_domain': True,\n",
       " 'is_robot_indexable': False,\n",
       " 'is_self': False,\n",
       " 'is_video': False,\n",
       " 'link_flair_background_color': '',\n",
       " 'link_flair_richtext': [],\n",
       " 'link_flair_text_color': 'dark',\n",
       " 'link_flair_type': 'text',\n",
       " 'locked': False,\n",
       " 'media_only': False,\n",
       " 'no_follow': True,\n",
       " 'num_comments': 2,\n",
       " 'num_crossposts': 0,\n",
       " 'over_18': False,\n",
       " 'parent_whitelist_status': 'all_ads',\n",
       " 'permalink': '/r/environment/comments/a70w7w/top_10_thing_you_might_not_know_about_pollution/',\n",
       " 'pinned': False,\n",
       " 'post_hint': 'image',\n",
       " 'preview': {'enabled': True,\n",
       "  'images': [{'id': 'UAq02RmvSvwLWB1MhXpaE4tfm9G0enYAOnBgD8OCuIQ',\n",
       "    'resolutions': [{'height': 152,\n",
       "      'url': 'https://preview.redd.it/r8e2jylnzu421.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=426658d6238f51fdec30160c9b594cf337871e81',\n",
       "      'width': 108}],\n",
       "    'source': {'height': 266,\n",
       "     'url': 'https://preview.redd.it/r8e2jylnzu421.jpg?auto=webp&amp;s=f527796fdd04cdc0a7ee5fa62807b9b3addbeef8',\n",
       "     'width': 189},\n",
       "    'variants': {}}]},\n",
       " 'pwls': 6,\n",
       " 'retrieved_on': 1545061542,\n",
       " 'score': 1,\n",
       " 'selftext': '',\n",
       " 'send_replies': True,\n",
       " 'spoiler': False,\n",
       " 'stickied': False,\n",
       " 'subreddit': 'environment',\n",
       " 'subreddit_id': 't5_2qh1n',\n",
       " 'subreddit_subscribers': 473332,\n",
       " 'subreddit_type': 'public',\n",
       " 'thumbnail': 'https://b.thumbs.redditmedia.com/Pb6EmIWVQD7Nwn7Nc_-WibL4Tr8SwcLgSK1fUhIsB7Q.jpg',\n",
       " 'thumbnail_height': 140,\n",
       " 'thumbnail_width': 140,\n",
       " 'title': 'Top 10 Thing You Might Not Know About Pollution',\n",
       " 'url': 'https://i.redd.it/r8e2jylnzu421.jpg',\n",
       " 'whitelist_status': 'all_ads',\n",
       " 'wls': 6}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()['data'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 7)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns = features)\n",
    "df1 = pd.DataFrame(columns = features)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvfile = 'environmentFinal.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "newparams = {'size': 500,\n",
    " 'subreddit': 'Environment',\n",
    " 'after': 1324000809,\n",
    " 'before': 1324260009}\n",
    "# For environment subreddit only: after variable is 24*3 hours interval from before variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'size': 500,\n",
       " 'subreddit': 'Environment',\n",
       " 'after': 1544839209,\n",
       " 'before': 1545098409}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156 rows to df 2011-12-19 02:00:09\n",
      "224 rows to df 2011-12-16 02:00:09\n",
      "160 rows to df 2011-12-13 02:00:09\n",
      "227 rows to df 2011-12-10 02:00:09\n",
      "226 rows to df 2011-12-07 02:00:09\n",
      "169 rows to df 2011-12-04 02:00:09\n",
      "223 rows to df 2011-12-01 02:00:09\n",
      "139 rows to df 2011-11-28 02:00:09\n",
      "171 rows to df 2011-11-25 02:00:09\n",
      "161 rows to df 2011-11-22 02:00:09\n",
      "211 rows to df 2011-11-19 02:00:09\n",
      "175 rows to df 2011-11-16 02:00:09\n",
      "208 rows to df 2011-11-13 02:00:09\n",
      "233 rows to df 2011-11-10 02:00:09\n",
      "153 rows to df 2011-11-07 02:00:09\n",
      "191 rows to df 2011-11-04 02:00:09\n",
      "156 rows to df 2011-11-01 02:00:09\n",
      "221 rows to df 2011-10-29 02:00:09\n",
      "181 rows to df 2011-10-26 02:00:09\n",
      "175 rows to df 2011-10-23 02:00:09\n",
      "206 rows to df 2011-10-20 02:00:09\n",
      "126 rows to df 2011-10-17 02:00:09\n",
      "179 rows to df 2011-10-14 02:00:09\n",
      "94 rows to df 2011-10-11 02:00:09\n",
      "169 rows to df 2011-10-08 02:00:09\n",
      "139 rows to df 2011-10-05 02:00:09\n",
      "156 rows to df 2011-10-02 02:00:09\n",
      "207 rows to df 2011-09-29 02:00:09\n",
      "129 rows to df 2011-09-26 02:00:09\n",
      "166 rows to df 2011-09-23 02:00:09\n",
      "159 rows to df 2011-09-20 02:00:09\n",
      "200 rows to df 2011-09-17 02:00:09\n",
      "146 rows to df 2011-09-14 02:00:09\n",
      "179 rows to df 2011-09-11 02:00:09\n",
      "195 rows to df 2011-09-08 02:00:09\n",
      "190 rows to df 2011-09-05 02:00:09\n",
      "232 rows to df 2011-09-02 02:00:09\n",
      "164 rows to df 2011-08-30 02:00:09\n",
      "221 rows to df 2011-08-27 02:00:09\n",
      "152 rows to df 2011-08-24 02:00:09\n",
      "204 rows to df 2011-08-21 02:00:09\n",
      "218 rows to df 2011-08-18 02:00:09\n",
      "135 rows to df 2011-08-15 02:00:09\n",
      "187 rows to df 2011-08-12 02:00:09\n",
      "176 rows to df 2011-08-09 02:00:09\n",
      "205 rows to df 2011-08-06 02:00:09\n",
      "160 rows to df 2011-08-03 02:00:09\n",
      "178 rows to df 2011-07-31 02:00:09\n",
      "171 rows to df 2011-07-28 02:00:09\n",
      "134 rows to df 2011-07-25 02:00:09\n",
      "269 rows to df 2011-07-22 02:00:09\n",
      "164 rows to df 2011-07-19 02:00:09\n",
      "265 rows to df 2011-07-16 02:00:09\n",
      "249 rows to df 2011-07-13 02:00:09\n",
      "223 rows to df 2011-07-10 02:00:09\n",
      "209 rows to df 2011-07-07 02:00:09\n",
      "171 rows to df 2011-07-04 02:00:09\n",
      "239 rows to df 2011-07-01 02:00:09\n",
      "153 rows to df 2011-06-28 02:00:09\n",
      "216 rows to df 2011-06-25 02:00:09\n",
      "179 rows to df 2011-06-22 02:00:09\n",
      "168 rows to df 2011-06-19 02:00:09\n",
      "229 rows to df 2011-06-16 02:00:09\n",
      "177 rows to df 2011-06-13 02:00:09\n",
      "271 rows to df 2011-06-10 02:00:09\n",
      "162 rows to df 2011-06-07 02:00:09\n",
      "238 rows to df 2011-06-04 02:00:09\n",
      "173 rows to df 2011-06-01 02:00:09\n",
      "185 rows to df 2011-05-29 02:00:09\n",
      "236 rows to df 2011-05-26 02:00:09\n",
      "147 rows to df 2011-05-23 02:00:09\n",
      "242 rows to df 2011-05-20 02:00:09\n",
      "151 rows to df 2011-05-17 02:00:09\n",
      "194 rows to df 2011-05-14 02:00:09\n",
      "183 rows to df 2011-05-11 02:00:09\n",
      "136 rows to df 2011-05-08 02:00:09\n",
      "200 rows to df 2011-05-05 02:00:09\n",
      "182 rows to df 2011-05-02 02:00:09\n",
      "226 rows to df 2011-04-29 02:00:09\n",
      "180 rows to df 2011-04-26 02:00:09\n",
      "181 rows to df 2011-04-23 02:00:09\n",
      "259 rows to df 2011-04-20 02:00:09\n",
      "216 rows to df 2011-04-17 02:00:09\n",
      "259 rows to df 2011-04-14 02:00:09\n",
      "181 rows to df 2011-04-11 02:00:09\n",
      "251 rows to df 2011-04-08 02:00:09\n",
      "242 rows to df 2011-04-05 02:00:09\n",
      "244 rows to df 2011-04-02 02:00:09\n",
      "263 rows to df 2011-03-30 02:00:09\n",
      "234 rows to df 2011-03-27 02:00:09\n",
      "292 rows to df 2011-03-24 02:00:09\n",
      "217 rows to df 2011-03-21 02:00:09\n",
      "258 rows to df 2011-03-18 02:00:09\n",
      "203 rows to df 2011-03-15 02:00:09\n",
      "234 rows to df 2011-03-12 02:00:09\n",
      "197 rows to df 2011-03-09 02:00:09\n",
      "200 rows to df 2011-03-06 02:00:09\n",
      "236 rows to df 2011-03-03 02:00:09\n",
      "175 rows to df 2011-02-28 02:00:09\n",
      "222 rows to df 2011-02-25 02:00:09\n",
      "171 rows to df 2011-02-22 02:00:09\n",
      "212 rows to df 2011-02-19 02:00:09\n",
      "180 rows to df 2011-02-16 02:00:09\n",
      "175 rows to df 2011-02-13 02:00:09\n",
      "232 rows to df 2011-02-10 02:00:09\n",
      "151 rows to df 2011-02-07 02:00:09\n",
      "268 rows to df 2011-02-04 02:00:09\n",
      "180 rows to df 2011-02-01 02:00:09\n",
      "260 rows to df 2011-01-29 02:00:09\n",
      "194 rows to df 2011-01-26 02:00:09\n",
      "243 rows to df 2011-01-23 02:00:09\n",
      "241 rows to df 2011-01-20 02:00:09\n",
      "155 rows to df 2011-01-17 02:00:09\n",
      "271 rows to df 2011-01-14 02:00:09\n",
      "195 rows to df 2011-01-11 02:00:09\n",
      "324 rows to df 2011-01-08 02:00:09\n",
      "232 rows to df 2011-01-05 02:00:09\n",
      "153 rows to df 2011-01-02 02:00:09\n",
      "183 rows to df 2010-12-30 02:00:09\n",
      "98 rows to df 2010-12-27 02:00:09\n",
      "212 rows to df 2010-12-24 02:00:09\n",
      "167 rows to df 2010-12-21 02:00:09\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'size': 500,\n",
       " 'subreddit': 'Environment',\n",
       " 'after': 1292378409,\n",
       " 'before': 1292637609}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addposts(url, newparams, csvfile=csvfile, df=df, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "newparams = {'size': 500,\n",
    " 'subreddit': 'environment',\n",
    " 'after': 1482180900,\n",
    " 'before': 1482353700}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1513303209"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1513562409-60*60*24*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1482180900-1481576100)/(3600*24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2016, 12, 17, 20, 55)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.utcfromtimestamp(1482008100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = 'cryptocurrency12_26_17to9_18_17.csv'\n",
    "# df.to_csv(file, mode='a', header=False)\n",
    "# df1.to_csv(file, mode='a', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
